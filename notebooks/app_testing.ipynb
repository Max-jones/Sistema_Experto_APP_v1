{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.data.load_dataset import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\elmha\\\\OneDrive - Universidad de Chile\\\\GitHub\\\\Sistema_Experto_APP_v1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_root(current_directory):\n",
    "    \"\"\"\n",
    "    Returns the project root directory for the project at the provided directory.\n",
    "\n",
    "    :returns: path to the root of the project\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "\n",
    "    home = str(Path.home())\n",
    "\n",
    "    while True:\n",
    "        list_ = os.listdir(current_directory)\n",
    "        parent_directory = os.path.dirname(current_directory)\n",
    "        if current_directory in (home, parent_directory):\n",
    "            break\n",
    "        if \".surround\" in list_:\n",
    "            return current_directory\n",
    "        current_directory = parent_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\elmha\\\\OneDrive - Universidad de Chile\\\\GitHub\\\\Sistema_Experto_APP_v1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory c:\\Users\\elmha\\OneDrive - Universidad de Chile\\GitHub\\Sistema_Experto_APP_v1\\notebooks\n",
      "Parent Directory c:\\Users\\elmha\\OneDrive - Universidad de Chile\\GitHub\\Sistema_Experto_APP_v1\n",
      "Data Directory c:\\Users\\elmha\\OneDrive - Universidad de Chile\\GitHub\\Sistema_Experto_APP_v1\\data\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(\"Current Directory\", path)\n",
    " \n",
    "parent_directory = os.path.dirname(path)\n",
    "print(\"Parent Directory\", parent_directory)\n",
    "\n",
    "data_directory = parent_directory + '\\data'\n",
    "print(\"Data Directory\", data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pres</th>\n",
       "      <th>pH</th>\n",
       "      <th>Temp</th>\n",
       "      <th>CE</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date_Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-02-03 13:00:00</th>\n",
       "      <td>1683.900000</td>\n",
       "      <td>7.409584</td>\n",
       "      <td>19.810000</td>\n",
       "      <td>301.531104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-03 14:00:00</th>\n",
       "      <td>1573.500001</td>\n",
       "      <td>7.409169</td>\n",
       "      <td>19.823333</td>\n",
       "      <td>304.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-03 15:00:00</th>\n",
       "      <td>1573.100003</td>\n",
       "      <td>7.411026</td>\n",
       "      <td>19.804000</td>\n",
       "      <td>304.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-03 16:00:00</th>\n",
       "      <td>1727.900006</td>\n",
       "      <td>7.605513</td>\n",
       "      <td>19.805714</td>\n",
       "      <td>311.796892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-03 17:00:00</th>\n",
       "      <td>1572.700009</td>\n",
       "      <td>7.407929</td>\n",
       "      <td>19.807778</td>\n",
       "      <td>302.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Pres        pH       Temp          CE  Label\n",
       "Date_Time                                                               \n",
       "2013-02-03 13:00:00  1683.900000  7.409584  19.810000  301.531104      0\n",
       "2013-02-03 14:00:00  1573.500001  7.409169  19.823333  304.666667      0\n",
       "2013-02-03 15:00:00  1573.100003  7.411026  19.804000  304.200000      0\n",
       "2013-02-03 16:00:00  1727.900006  7.605513  19.805714  311.796892      0\n",
       "2013-02-03 17:00:00  1572.700009  7.407929  19.807778  302.600000      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = 'C:\\\\Users\\\\elmha\\\\OneDrive - Universidad de Chile\\\\GitHub\\\\Sistema_Experto_APP_v1\\\\data\\processed\\\\pozo_1_procesado.csv'\n",
    "df = load_data(local_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elmha\\OneDrive - Universidad de Chile\\GitHub\\Sistema_Experto_APP_v1\\venv\\lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:753: UserWarning: Parsing '15-01-2017' in DD/MM/YYYY format. Provide format or specify infer_datetime_format=True for consistent parsing.\n",
      "  end_casted = self._maybe_cast_slice_bound(end, \"right\")\n"
     ]
    }
   ],
   "source": [
    "# time_window = df.loc['15-08-2015':'15-01-2017']\n",
    "\n",
    "time_window = df.loc[:'15-01-2017']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- ![Ventana](ventana.jpg) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2018-2019 Streamlit Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# %% Imports -> requeriments.txt\n",
    "\n",
    "# Funcionalidades de la aplicación\n",
    "from numpy.lib.function_base import select\n",
    "# import streamlit as st\n",
    "# import pandas_profiling\n",
    "# from streamlit_pandas_profiling import st_profile_report\n",
    "\n",
    "\n",
    "# import pandas_profiling\n",
    "\n",
    "# Manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np ##\n",
    "import altair as alt ##\n",
    "import pydeck as pdk ##\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from streamlit.proto.DataFrame_pb2 import DataFrame\n",
    "\n",
    "# Manejo del tiempo\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.figure_factory as ff\n",
    "# import plotly.express as px\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# Clasificadores \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Automated Classification\n",
    "from pycaret import classification as supervised\n",
    "from pycaret import anomaly as unsupervised\n",
    "# import pycaret.anomaly as unsupervised\n",
    "\n",
    "\n",
    "### Initial Confiugurations\n",
    "# SETTING PAGE CONFIG TO WIDE MODE\n",
    "# st.set_page_config(\n",
    "#     layout=\"wide\",\n",
    "#     page_title=\"Plataforma automática para detección de anomalías\",\n",
    "#     page_icon=\"🚀\",\n",
    "#     initial_sidebar_state=\"expanded\",\n",
    "# )\n",
    "\n",
    "# # LOADING LOCAL DATA IF EXISTS.\n",
    "# # local_path = \"C:\\\\Users\\elmha\\OneDrive - Universidad de Chile\\Magíster\\Tesis\\Sistema-Experto\\Data\\processed/dataframe.csv\"\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    ARGS: path to the local .csv file\n",
    "    Load data and search for the Date_Time column to index the dataframe by a datetime value.\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path,delimiter=\";\")  # , engine='python')\n",
    "    data[\"Date_Time\"] = pd.to_datetime(data[\"Date_Time\"])\n",
    "    data.set_index(\"Date_Time\", inplace=True)\n",
    "    chile = pytz.timezone(\"Chile/Continental\")\n",
    "    data.index = data.index.tz_localize(pytz.utc).tz_convert(chile)\n",
    "    return data\n",
    "\n",
    "def show_cv_iterations(n_splits, X, y, timeseries=True):\n",
    "    # https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n",
    "    if timeseries:\n",
    "        cv = TimeSeriesSplit(n_splits)\n",
    "    else:\n",
    "        cv = KFold(n_splits)\n",
    "    \n",
    "    figure, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "        \n",
    "        p1 = ax.scatter(tr, [ii] * len(tr), c='black', marker=\"_\", lw=8)\n",
    "        p2 = ax.scatter(tt, [ii] * len(tt), c='red', marker=\"_\", lw=8)\n",
    "        ax.set(\n",
    "            title=\"Behavior of TimeseriesSplit\",\n",
    "            xlabel=\"Data Index\",\n",
    "            ylabel=\"CV Iteration\",\n",
    "            ylim=[5, -1],\n",
    "        )\n",
    "        ax.legend([p1, p2], [\"Training\", \"Validation\"])\n",
    "    st.pyplot(fig=figure)\n",
    "    return cv\n",
    "\n",
    "# @st.cache\n",
    "def entrenar_modelos(df, etiqueta, metrica, ensamble=True):\n",
    "    \n",
    "\n",
    "    # setup\n",
    "    pycaret_s = supervised.setup(df, target = etiqueta, session_id = 123, silent = True, use_gpu = True, profile = False)     \n",
    "    # model training and selection\n",
    "    if ensamble:\n",
    "        top5 = supervised.compare_models(n_select = 5) \n",
    "        # tune top 5 base models\n",
    "        grid_a= supervised.pull()\n",
    "        tuned_top5 = [supervised.tune_model(i,fold = 5, optimize='F1',search_library='scikit-optimize') for i in top5]\n",
    "        grid_b=supervised.pull()\n",
    "        stacker = supervised.stack_models(estimator_list = top5[1:], meta_model = top5[0])\n",
    "\n",
    "        # \n",
    "        return (stacker, grid_a, grid_b)\n",
    "    else:\n",
    "        best = supervised.compare_models(sort= metrica, n_select=3)\n",
    "        grid = supervised.pull()\n",
    "        return (best, grid, grid)\n",
    "    \n",
    "def deteccion_no_supervisada(df, metrica, etiqueta=None,  ensamble=True):\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def cargar_modelo(df,modelo):\n",
    "\n",
    "    modelo = supervised.load_model('stack inicial')\n",
    "    \n",
    "    return (modelo, grid)\n",
    "\n",
    "# Creando las secciones de visualización de la aplicación\n",
    "\n",
    "# Título de la plataforma\n",
    "\"\"\"\n",
    "# Sistema Experto - Plataforma WEB para detección de anomalías\n",
    "\"\"\"\n",
    "\n",
    "st.sidebar.write(\"## Menu de pre-configuración\")\n",
    "st.sidebar.write(\n",
    "\"\"\"\n",
    "### 1️⃣ Cargar el dataset a procesar\n",
    "\"\"\"\n",
    ")\n",
    " \n",
    "# Sección de carga del archivo .csv\n",
    "\n",
    "# Widget para cargar el archivo\n",
    "uploaded_file = st.sidebar.file_uploader(\"Selecciona un archivo .csv \")\n",
    "\n",
    "# La aplicación comienza cuando se carga un archivo.\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    uploaded_file.seek(0)\n",
    "\n",
    "    # Se carga el archivo\n",
    "    ds = load_data(uploaded_file)\n",
    "\n",
    "    # Confirmación carga archivo\n",
    "    st.sidebar.write(\"**Se ha cargado un archivo.**\")\n",
    "\n",
    "    # Se extraen los nombres de las columnas del dataset cargado.\n",
    "    columns_names_list = ds.columns.to_list()\n",
    "    st.sidebar.write(columns_names_list)\n",
    "\n",
    "    # Widget para seleccionar las variables monitoreadas a analizar.\n",
    "    st.sidebar.write(\n",
    "    \"\"\"\n",
    "    ### 2️⃣ Seleccione los nombres de las columnas que contienen características\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    selected_features = st.sidebar.multiselect(\n",
    "        \" Seleccione las características\",\n",
    "        columns_names_list,\n",
    "    )\n",
    "    \n",
    "    # Widget de consulta si el dataset contiene etiquetas.\n",
    "    labeled = st.sidebar.selectbox(\n",
    "        \"¿El dataset posee etiquetas?\",\n",
    "        [\"Seleccione una opción ✔️\",\"Sí\", \"No\"],\n",
    "        help=\"Esta pregunta se refiere si la base de datos cargada contiene una columna con la información si los datos han sido etiquetados previamente como datos normales y anómalos.\",\n",
    "    )\n",
    "\n",
    "    if labeled == \"Sí\":\n",
    "        target = st.sidebar.selectbox(\n",
    "            \"Ingrese el nombre de la columna que contiene las etiquetas.\",\n",
    "            columns_names_list,\n",
    "            help=\"Esta columna debe ser de tipo binario. Donde 0 corresponde a un dato normal y 1 a una medición anómala.\",\n",
    "            index=len(columns_names_list)-1\n",
    "        )\n",
    "\n",
    "    elif labeled == \"Seleccione una opción✅\":  \n",
    "        st.sidebar.write(\"Las preguntas anteriores son obligatorias.\")  \n",
    "\n",
    "\n",
    "    ready = st.sidebar.button(\"Comenzar!\")\n",
    "\n",
    "    if ready:\n",
    " \n",
    "        selected_df = ds[selected_features]\n",
    "        if labeled == 'Sí':\n",
    "            selected_df['target'] = ds[target]\n",
    "        \n",
    "        complete_df = selected_df\n",
    "       \n",
    "        \n",
    "        # if st.button(\"Generar un reporte exploratorio inicial 🕵️\"):\n",
    "\n",
    "            # if st.button('Generar reporte'):\n",
    "            #     with st.spinner(\"Training ongoing\"):\n",
    "            #         time.sleep(3)\n",
    "        st.write('## Análisis exploratorio estadístico y visual de los datos cargados: ')\n",
    "        with st.expander(\"🕵️ Mostrar un reporte exploratorio preliminar 📃\", expanded=False):\n",
    "        \n",
    "            # st.write(selected_df)  # use_container_width=True)\n",
    "            pr = complete_df.profile_report()\n",
    "            # profile = ProfileReport(pr, title=\"Reporte de exploración de datos\")\n",
    "\n",
    "            st_profile_report(pr)\n",
    "            # else:\n",
    "            #     st.write('🚧 Por favor seleccione primero las variables a analizar 🚧. ')\n",
    "        # else:\n",
    "        #     pass\n",
    "            \n",
    "\n",
    "# %% Separación de los conjuntos de entrenamiento y validación\n",
    "        \n",
    "        st.write('## Detección de anomalías')\n",
    "        if labeled:\n",
    "            antes = time.time()\n",
    "            best, grid1, grid2  = entrenar_modelos(complete_df, 'target', 'F1')\n",
    "            despues = time.time()\n",
    "            delta_t = despues- antes\n",
    "            str_t = 'El entrenamiento demoró: '+str(delta_t) + ' segundos.'\n",
    "            st.write(str_t)\n",
    "            # pycaret_s = setup(complete_df, target = 'target', session_id = 123, silent = True, use_gpu = True, profile = False)     \n",
    "            # model training and selection\n",
    "            # best = compare_models(sort='F1')#,n_select=3)\n",
    "            # score_grid = pull()\n",
    "            st.write('### Grilla de búsqueda de modelos:')\n",
    "            st.write(grid1)\n",
    "            # st.write(grid2)\n",
    "\n",
    "            st.write('### Apilamiento de los mejors 5 modelos con mejor desempeño:')\n",
    "            st.write('# Los mejores clasificador fueron:')\n",
    "            # st.write(supervised.pull())\n",
    "            \n",
    "\n",
    "        # # Guardar modelos\n",
    "        # save_model(best, 'app_best')\n",
    "        # st.write(score_grid)\n",
    "        \n",
    "        supervised.plot_model(best,plot = 'class_report',display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'confusion_matrix',display_format='streamlit',plot_kwargs = {'percent' : True})\n",
    "        supervised.plot_model(best,plot = 'error', display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'pr', display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'boundary',display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'calibration',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'vc',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'feature',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'feature_all',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'parameter',display_format='streamlit')\n",
    "        \n",
    "        \n",
    "        leaderboard = supervised.get_leaderboard()\n",
    "        # st.write('Dashboard Resultados:')\n",
    "        # ds = dashboard(best, display_format='inline')\n",
    "\n",
    "\n",
    "        # X = selected_df\n",
    "        # y = ds[target]\n",
    "        # st.header('Entrenamiento de modelos')\n",
    "        # # st.write(X)\n",
    "        # # st.write(y)\n",
    "        # tscv = show_cv_iterations(5,X,y)\n",
    "\n",
    "\n",
    "\n",
    "# %% Comparación de modelos\n",
    "\n",
    "        # suppervised_classifiers = [\n",
    "        #     KNeighborsClassifier(3),\n",
    "        #     SVC(probability=True),\n",
    "        #     DecisionTreeClassifier(),\n",
    "        #     RandomForestClassifier(),\n",
    "        #     AdaBoostClassifier(),\n",
    "        #     GradientBoostingClassifier(),\n",
    "        #     GaussianNB(),\n",
    "        #     LinearDiscriminantAnalysis(),\n",
    "        #     QuadraticDiscriminantAnalysis(),\n",
    "        #     LogisticRegression()]\n",
    "\n",
    "        \n",
    "        # log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "        # log \t = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "        # acc_dict = {}\n",
    "\n",
    "        # for train_index, test_index in tscv.split(X):\n",
    "        #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        #     X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "        #     y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "\n",
    "        #     for clf in suppervised_classifiers:\n",
    "        #         # plo\n",
    "        #         name = clf.__class__.__name__\n",
    "        #         clf.fit(X_train, y_train)\n",
    "        #         train_predictions = clf.predict(X_test)\n",
    "        #         acc = accuracy_score(y_test, train_predictions)\n",
    "        #         if name in acc_dict:\n",
    "        #             acc_dict[name] += acc\n",
    "        #         else:\n",
    "        #             acc_dict[name] = acc\n",
    "\n",
    "        # for clf in acc_dict:\n",
    "        #     acc_dict[clf] = acc_dict[clf] / 10.0\n",
    "        #     log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "        #     log = log.append(log_entry)\n",
    "        \n",
    "        # plt.xlabel('Accuracy')\n",
    "        # plt.title('Classifier Accuracy')\n",
    "\n",
    "        # results_fig = plt.figure()\n",
    "        # sns.set_color_codes(\"muted\")\n",
    "        # sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "        # st.pyplot(results_fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas_profiling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstreamlit_pandas_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m st_profile_report\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# import pandas_profiling\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Manejo de datos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas_profiling'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2018-2019 Streamlit Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# %% Imports -> requeriments.txt\n",
    "\n",
    "# Funcionalidades de la aplicación\n",
    "from numpy.lib.function_base import select\n",
    "# import streamlit as st\n",
    "# import pandas_profiling\n",
    "# from streamlit_pandas_profiling import st_profile_report\n",
    "\n",
    "\n",
    "# import pandas_profiling\n",
    "\n",
    "# Manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np ##\n",
    "import altair as alt ##\n",
    "import pydeck as pdk ##\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# from streamlit.proto.DataFrame_pb2 import DataFrame\n",
    "\n",
    "# Manejo del tiempo\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.figure_factory as ff\n",
    "# import plotly.express as px\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "# Clasificadores \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# Automated Classification\n",
    "from pycaret import classification as supervised\n",
    "from pycaret import anomaly as unsupervised\n",
    "# import pycaret.anomaly as unsupervised\n",
    "\n",
    "\n",
    "### Initial Confiugurations\n",
    "# SETTING PAGE CONFIG TO WIDE MODE\n",
    "# st.set_page_config(\n",
    "#     layout=\"wide\",\n",
    "#     page_title=\"Plataforma automática para detección de anomalías\",\n",
    "#     page_icon=\"🚀\",\n",
    "#     initial_sidebar_state=\"expanded\",\n",
    "# )\n",
    "\n",
    "# # LOADING LOCAL DATA IF EXISTS.\n",
    "# # local_path = \"C:\\\\Users\\elmha\\OneDrive - Universidad de Chile\\Magíster\\Tesis\\Sistema-Experto\\Data\\processed/dataframe.csv\"\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    ARGS: path to the local .csv file\n",
    "    Load data and search for the Date_Time column to index the dataframe by a datetime value.\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path,delimiter=\";\")  # , engine='python')\n",
    "    data[\"Date_Time\"] = pd.to_datetime(data[\"Date_Time\"])\n",
    "    data.set_index(\"Date_Time\", inplace=True)\n",
    "    chile = pytz.timezone(\"Chile/Continental\")\n",
    "    data.index = data.index.tz_localize(pytz.utc).tz_convert(chile)\n",
    "    return data\n",
    "\n",
    "def show_cv_iterations(n_splits, X, y, timeseries=True):\n",
    "    # https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4\n",
    "    if timeseries:\n",
    "        cv = TimeSeriesSplit(n_splits)\n",
    "    else:\n",
    "        cv = KFold(n_splits)\n",
    "    \n",
    "    figure, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "        \n",
    "        p1 = ax.scatter(tr, [ii] * len(tr), c='black', marker=\"_\", lw=8)\n",
    "        p2 = ax.scatter(tt, [ii] * len(tt), c='red', marker=\"_\", lw=8)\n",
    "        ax.set(\n",
    "            title=\"Behavior of TimeseriesSplit\",\n",
    "            xlabel=\"Data Index\",\n",
    "            ylabel=\"CV Iteration\",\n",
    "            ylim=[5, -1],\n",
    "        )\n",
    "        ax.legend([p1, p2], [\"Training\", \"Validation\"])\n",
    "    st.pyplot(fig=figure)\n",
    "    return cv\n",
    "\n",
    "# @st.cache\n",
    "def entrenar_modelos(df, etiqueta, metrica, ensamble=True):\n",
    "    \n",
    "\n",
    "    # setup\n",
    "    pycaret_s = supervised.setup(df, target = etiqueta, session_id = 123, silent = True, use_gpu = True, profile = False)     \n",
    "    # model training and selection\n",
    "    if ensamble:\n",
    "        top5 = supervised.compare_models(n_select = 5) \n",
    "        # tune top 5 base models\n",
    "        grid_a= supervised.pull()\n",
    "        tuned_top5 = [supervised.tune_model(i,fold = 5, optimize='F1',search_library='scikit-optimize') for i in top5]\n",
    "        grid_b=supervised.pull()\n",
    "        stacker = supervised.stack_models(estimator_list = top5[1:], meta_model = top5[0])\n",
    "\n",
    "        # \n",
    "        return (stacker, grid_a, grid_b)\n",
    "    else:\n",
    "        best = supervised.compare_models(sort= metrica, n_select=3)\n",
    "        grid = supervised.pull()\n",
    "        return (best, grid, grid)\n",
    "    \n",
    "def deteccion_no_supervisada(df, metrica, etiqueta=None,  ensamble=True):\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def cargar_modelo(df,modelo):\n",
    "\n",
    "    modelo = supervised.load_model('stack inicial')\n",
    "    \n",
    "    return (modelo, grid)\n",
    "\n",
    "# Creando las secciones de visualización de la aplicación\n",
    "\n",
    "# Título de la plataforma\n",
    "\"\"\"\n",
    "# Sistema Experto - Plataforma WEB para detección de anomalías\n",
    "\"\"\"\n",
    "\n",
    "st.sidebar.write(\"## Menu de pre-configuración\")\n",
    "st.sidebar.write(\n",
    "\"\"\"\n",
    "### 1️⃣ Cargar el dataset a procesar\n",
    "\"\"\"\n",
    ")\n",
    " \n",
    "# Sección de carga del archivo .csv\n",
    "\n",
    "# Widget para cargar el archivo\n",
    "uploaded_file = st.sidebar.file_uploader(\"Selecciona un archivo .csv \")\n",
    "\n",
    "# La aplicación comienza cuando se carga un archivo.\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    uploaded_file.seek(0)\n",
    "\n",
    "    # Se carga el archivo\n",
    "    ds = load_data(uploaded_file)\n",
    "\n",
    "    # Confirmación carga archivo\n",
    "    st.sidebar.write(\"**Se ha cargado un archivo.**\")\n",
    "\n",
    "    # Se extraen los nombres de las columnas del dataset cargado.\n",
    "    columns_names_list = ds.columns.to_list()\n",
    "    st.sidebar.write(columns_names_list)\n",
    "\n",
    "    # Widget para seleccionar las variables monitoreadas a analizar.\n",
    "    st.sidebar.write(\n",
    "    \"\"\"\n",
    "    ### 2️⃣ Seleccione los nombres de las columnas que contienen características\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "    selected_features = st.sidebar.multiselect(\n",
    "        \" Seleccione las características\",\n",
    "        columns_names_list,\n",
    "    )\n",
    "    \n",
    "    # Widget de consulta si el dataset contiene etiquetas.\n",
    "    labeled = st.sidebar.selectbox(\n",
    "        \"¿El dataset posee etiquetas?\",\n",
    "        [\"Seleccione una opción ✔️\",\"Sí\", \"No\"],\n",
    "        help=\"Esta pregunta se refiere si la base de datos cargada contiene una columna con la información si los datos han sido etiquetados previamente como datos normales y anómalos.\",\n",
    "    )\n",
    "\n",
    "    if labeled == \"Sí\":\n",
    "        target = st.sidebar.selectbox(\n",
    "            \"Ingrese el nombre de la columna que contiene las etiquetas.\",\n",
    "            columns_names_list,\n",
    "            help=\"Esta columna debe ser de tipo binario. Donde 0 corresponde a un dato normal y 1 a una medición anómala.\",\n",
    "            index=len(columns_names_list)-1\n",
    "        )\n",
    "\n",
    "    elif labeled == \"Seleccione una opción✅\":  \n",
    "        st.sidebar.write(\"Las preguntas anteriores son obligatorias.\")  \n",
    "\n",
    "\n",
    "    ready = st.sidebar.button(\"Comenzar!\")\n",
    "\n",
    "    if ready:\n",
    " \n",
    "        selected_df = ds[selected_features]\n",
    "        if labeled == 'Sí':\n",
    "            selected_df['target'] = ds[target]\n",
    "        \n",
    "        complete_df = selected_df\n",
    "       \n",
    "        \n",
    "        # if st.button(\"Generar un reporte exploratorio inicial 🕵️\"):\n",
    "\n",
    "            # if st.button('Generar reporte'):\n",
    "            #     with st.spinner(\"Training ongoing\"):\n",
    "            #         time.sleep(3)\n",
    "        st.write('## Análisis exploratorio estadístico y visual de los datos cargados: ')\n",
    "        with st.expander(\"🕵️ Mostrar un reporte exploratorio preliminar 📃\", expanded=False):\n",
    "        \n",
    "            # st.write(selected_df)  # use_container_width=True)\n",
    "            pr = complete_df.profile_report()\n",
    "            # profile = ProfileReport(pr, title=\"Reporte de exploración de datos\")\n",
    "\n",
    "            st_profile_report(pr)\n",
    "            # else:\n",
    "            #     st.write('🚧 Por favor seleccione primero las variables a analizar 🚧. ')\n",
    "        # else:\n",
    "        #     pass\n",
    "            \n",
    "\n",
    "# %% Separación de los conjuntos de entrenamiento y validación\n",
    "        \n",
    "        st.write('## Detección de anomalías')\n",
    "        if labeled:\n",
    "            antes = time.time()\n",
    "            best, grid1, grid2  = entrenar_modelos(complete_df, 'target', 'F1')\n",
    "            despues = time.time()\n",
    "            delta_t = despues- antes\n",
    "            str_t = 'El entrenamiento demoró: '+str(delta_t) + ' segundos.'\n",
    "            st.write(str_t)\n",
    "            # pycaret_s = setup(complete_df, target = 'target', session_id = 123, silent = True, use_gpu = True, profile = False)     \n",
    "            # model training and selection\n",
    "            # best = compare_models(sort='F1')#,n_select=3)\n",
    "            # score_grid = pull()\n",
    "            st.write('### Grilla de búsqueda de modelos:')\n",
    "            st.write(grid1)\n",
    "            # st.write(grid2)\n",
    "\n",
    "            st.write('### Apilamiento de los mejors 5 modelos con mejor desempeño:')\n",
    "            st.write('# Los mejores clasificador fueron:')\n",
    "            # st.write(supervised.pull())\n",
    "            \n",
    "\n",
    "        # # Guardar modelos\n",
    "        # save_model(best, 'app_best')\n",
    "        # st.write(score_grid)\n",
    "        \n",
    "        supervised.plot_model(best,plot = 'class_report',display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'confusion_matrix',display_format='streamlit',plot_kwargs = {'percent' : True})\n",
    "        supervised.plot_model(best,plot = 'error', display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'pr', display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'boundary',display_format='streamlit')\n",
    "        supervised.plot_model(best,plot = 'calibration',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'vc',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'feature',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'feature_all',display_format='streamlit')\n",
    "        # supervised.plot_model(best,plot = 'parameter',display_format='streamlit')\n",
    "        \n",
    "        \n",
    "        leaderboard = supervised.get_leaderboard()\n",
    "        # st.write('Dashboard Resultados:')\n",
    "        # ds = dashboard(best, display_format='inline')\n",
    "\n",
    "\n",
    "        # X = selected_df\n",
    "        # y = ds[target]\n",
    "        # st.header('Entrenamiento de modelos')\n",
    "        # # st.write(X)\n",
    "        # # st.write(y)\n",
    "        # tscv = show_cv_iterations(5,X,y)\n",
    "\n",
    "\n",
    "\n",
    "# %% Comparación de modelos\n",
    "\n",
    "        # suppervised_classifiers = [\n",
    "        #     KNeighborsClassifier(3),\n",
    "        #     SVC(probability=True),\n",
    "        #     DecisionTreeClassifier(),\n",
    "        #     RandomForestClassifier(),\n",
    "        #     AdaBoostClassifier(),\n",
    "        #     GradientBoostingClassifier(),\n",
    "        #     GaussianNB(),\n",
    "        #     LinearDiscriminantAnalysis(),\n",
    "        #     QuadraticDiscriminantAnalysis(),\n",
    "        #     LogisticRegression()]\n",
    "\n",
    "        \n",
    "        # log_cols = [\"Classifier\", \"Accuracy\"]\n",
    "        # log \t = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "        # acc_dict = {}\n",
    "\n",
    "        # for train_index, test_index in tscv.split(X):\n",
    "        #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        #     X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "        #     y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "\n",
    "        #     for clf in suppervised_classifiers:\n",
    "        #         # plo\n",
    "        #         name = clf.__class__.__name__\n",
    "        #         clf.fit(X_train, y_train)\n",
    "        #         train_predictions = clf.predict(X_test)\n",
    "        #         acc = accuracy_score(y_test, train_predictions)\n",
    "        #         if name in acc_dict:\n",
    "        #             acc_dict[name] += acc\n",
    "        #         else:\n",
    "        #             acc_dict[name] = acc\n",
    "\n",
    "        # for clf in acc_dict:\n",
    "        #     acc_dict[clf] = acc_dict[clf] / 10.0\n",
    "        #     log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n",
    "        #     log = log.append(log_entry)\n",
    "        \n",
    "        # plt.xlabel('Accuracy')\n",
    "        # plt.title('Classifier Accuracy')\n",
    "\n",
    "        # results_fig = plt.figure()\n",
    "        # sns.set_color_codes(\"muted\")\n",
    "        # sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "        # st.pyplot(results_fig)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c09ac1c871fab6ce729240e4117f4410ed25efdabd292c5f4dc7e995cc0ad748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
